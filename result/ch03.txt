 uv run python3 ch03.py

======================================================================
Chapter 3 - Looking Inside Transformer LLMs
Hands-On Large Language Models - macOS Edition
======================================================================

This demo covers 3 key concepts:
1. Loading and Inspecting LLM Architecture
2. Model Inputs, Outputs, and Token Generation
3. KV Cache Optimization

======================================================================
PART 1: Loading and Inspecting LLM Architecture
======================================================================

[1/3] Loading Phi-3 model from local directory...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|██████| 4/4 [00:15<00:00,  3.91s/it]
✓ Model loaded successfully

[2/3] Creating text generation pipeline...
Device set to use mps
The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
✓ Pipeline created

[3/3] Generating sample text...

Prompt: Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.

Generated text:
 Mention that you've taken steps to prevent it in the future.


Email to Sarah:

Subject: Sincere Apologies for the Gardening Mishap


Dear Sarah,


I hope

----------------------------------------------------------------------
Model Architecture Overview:
----------------------------------------------------------------------
Phi3ForCausalLM(
  (model): Phi3Model(
    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        (self_attn): Phi3Attention(
          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)
        )
        (mlp): Phi3MLP(
          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLUActivation()
        )
        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)
        (resid_attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
      )
    )
    (norm): Phi3RMSNorm((3072,), eps=1e-05)
    (rotary_emb): Phi3RotaryEmbedding()
  )
  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)
)

✓ Part 1 Complete

======================================================================
PART 2: Model Inputs, Outputs, and Token Generation
======================================================================

Prompt: 'The capital of France is'

[1/4] Tokenizing input...
Input token IDs: [450, 7483, 310, 3444, 338]
Input shape: torch.Size([1, 5]) (batch_size=1, sequence_length=5)

[2/4] Running through transformer layers (model.model)...
Model output shape: torch.Size([1, 5, 3072])
  - Batch size: 1
  - Sequence length: 5
  - Hidden dimension: 3072

[3/4] Running through language model head (lm_head)...
LM head output shape: torch.Size([1, 5, 32064])
  - Batch size: 1
  - Sequence length: 5
  - Vocabulary size: 32064

[4/4] Selecting next token (greedy decoding)...
Next token ID: 3681
Next token text: 'Paris'

Complete output: 'The capital of France is Paris'

Top 5 predicted tokens:
  1. 'Paris' (probability: 0.8861)
  2. '_' (probability: 0.0259)
  3. 'not' (probability: 0.0111)
  4. '...' (probability: 0.0067)
  5. '
' (probability: 0.0057)

✓ Part 2 Complete

======================================================================
PART 3: KV Cache Optimization
======================================================================

Prompt: Write a very long email apologizing to Sarah for the tragic ...

Generating 100 tokens with different cache settings...

[1/2] Testing WITH KV cache (use_cache=True)...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
✓ Generation complete in 7.57 seconds
Generated 100 tokens

[2/2] Testing WITHOUT KV cache (use_cache=False)...
✓ Generation complete in 20.03 seconds
Generated 100 tokens

----------------------------------------------------------------------
Performance Comparison:
----------------------------------------------------------------------
WITH cache:    7.57 seconds
WITHOUT cache: 20.03 seconds
Speedup:       2.65x faster with cache
Time saved:    12.46 seconds (62.2% reduction)

----------------------------------------------------------------------
Why KV Cache Helps:
----------------------------------------------------------------------
• WITHOUT cache: Recomputes attention for ALL previous tokens at each step
• WITH cache:    Reuses cached key/value matrices from previous tokens
• Result:        O(n²) → O(n) complexity reduction
----------------------------------------------------------------------

Generated text (first 200 chars):
Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. Mention that you've been researching ways to prevent such accidents in the future. Include a deta...

✓ Part 3 Complete

======================================================================
All demos complete!
======================================================================