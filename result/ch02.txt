$D uv run python ch02.py


======================================================================
Chapter 2 - Tokens and Token Embeddings
Hands-On Large Language Models - macOS Edition
======================================================================

This demo covers 5 key concepts:
1. LLM Tokenization and Text Generation
2. Comparing Different Tokenizers
3. Contextualized Word Embeddings
4. Sentence Embeddings
5. Static Word Embeddings and Applications

======================================================================
PART 1: LLM Tokenization and Text Generation
======================================================================

[1/3] Loading Phi-3 model and tokenizer from local directory...
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards: 100%|███████████████████████| 4/4 [00:13<00:00,  3.44s/it]
✓ Model loaded successfully

[2/3] Generating text...
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.

Generated text:
Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Heartfelt Apologies for the Gardening Mishap


Dear Sarah,


I hope this message finds you well. I am writing to express my deepest apologies for the unfortunate incident that

[3/3] Understanding tokenization...

Original prompt: Write an email apologizing to Sarah for the tragic...

Token IDs: [14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305]...

Individual tokens:
  Token 0: ID=14350 → 'Write'
  Token 1: ID=  385 → 'an'
  Token 2: ID= 4876 → 'email'
  Token 3: ID=27746 → 'apolog'
  Token 4: ID= 5281 → 'izing'
  Token 5: ID=  304 → 'to'
  Token 6: ID=19235 → 'Sarah'
  Token 7: ID=  363 → 'for'
  Token 8: ID=  278 → 'the'
  Token 9: ID=25305 → 'trag'

✓ Part 1 Complete

======================================================================
PART 2: Comparing Different Tokenizers
======================================================================

Test text: English CAPITALIZATION show_tokens False None elif == >= 12.0*50=600


--- bert-base-uncased ---
Tokens (24): [CLS] | english | capital | ##ization | show | _ | token | ##s | false | none | eli | ##f | = | = | > | = | 12 | . | 0 | * | 50 | = | 600 | [SEP]

--- bert-base-cased ---
Tokens (32): [CLS] | English | CA | ##PI | ##TA | ##L | ##I | ##Z | ##AT | ##ION | show | _ | token | ##s | F | ##als | ##e | None | el | ##if | = | = | > | = | 12 | . | 0 | * | 50 | = | 600 | [SEP]

--- gpt2 ---
Tokens (23): English |  CAP | ITAL | IZ | ATION |  show | _ | t | ok | ens |  False |  None |  el | if |  == |  >= |  12 | . | 0 | * | 50 | = | 600

--- Phi-3 ---
Tokens (28): English | C | AP | IT | AL | IZ | ATION | show | _ | to | kens | False | None | elif | == | >= |  | 1 | 2 | . | 0 | * | 5 | 0 | = | 6 | 0 | 0

✓ Part 2 Complete

======================================================================
PART 3: Contextualized Word Embeddings
======================================================================

[1/2] Loading DeBERTa model...
✓ Model loaded

[2/2] Getting contextualized embeddings...

Input text: 'Hello world'
Output shape: torch.Size([1, 4, 384])
  - Batch size: 1
  - Number of tokens: 4
  - Embedding dimension: 384

Tokens:
  Token 0: '[CLS]' → [-3.4816, 0.0861, ...]
  Token 1: 'Hello' → [0.1898, 0.3208, ...]
  Token 2: ' world' → [0.2071, 0.5036, ...]
  Token 3: '[SEP]' → [-3.4278, 0.0645, ...]

✓ Part 3 Complete

======================================================================
PART 4: Sentence Embeddings
======================================================================

[1/2] Loading sentence transformer model...
✓ Model loaded

[2/2] Encoding sentences...

Encoded 3 sentences
Embedding shape per sentence: (768,)

Sentence 1: 'Best movie ever!'
  Embedding (first 5 dims): [-0.0202, 0.0458, -0.0127, ...]

Sentence 2: 'This film was amazing!'
  Embedding (first 5 dims): [-0.0330, 0.0252, -0.0221, ...]

Sentence 3: 'I love programming in Python.'
  Embedding (first 5 dims): [-0.0047, 0.0550, -0.0263, ...]

Similarity between sentence 1 and 2: 0.6352
Similarity between sentence 1 and 3: 0.1232
(Higher similarity = more semantically similar)

✓ Part 4 Complete

======================================================================
PART 5: Static Word Embeddings and Applications
======================================================================

--- Part 5a: GloVe Word Embeddings ---
Loading GloVe embeddings (cached by gensim)...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
✓ GloVe embeddings loaded

Words most similar to 'king':
  king            (similarity: 1.0000)
  prince          (similarity: 0.8236)
  queen           (similarity: 0.7839)
  ii              (similarity: 0.7746)
  emperor         (similarity: 0.7736)
  son             (similarity: 0.7667)
  uncle           (similarity: 0.7627)
  kingdom         (similarity: 0.7542)
  throne          (similarity: 0.7540)
  brother         (similarity: 0.7492)


--- Part 5b: Song Recommendation System ---
Downloading playlist dataset...
✓ Loaded 11088 playlists
Training Word2Vec model on playlists...
✓ Model trained

--- Song Recommendations ---

Input song: 'Fade To Black' by Metallica
Recommended songs:
  • 'Run To The Hills' by Iron Maiden
  • 'Three Lock Box' by Sammy Hagar
  • 'Little Guitars' by Van Halen
  • 'Mr. Brownstone' by Guns N' Roses
  • 'I Don't Know' by Ozzy Osbourne

Input song: 'California Love (w\/ Dr. Dre & Roger Troutman)' by 2Pac
Recommended songs:
  • 'If I Ruled The World (Imagine That) (w\/ Lauryn Hill)' by Nas
  • 'Hate It Or Love It (w\/ 50 Cent)' by The Game
  • 'Drop It Like It's Hot (w\/ Pharrell)' by Snoop Dogg
  • 'Out Of My Head (w\/ Trey Songz)' by Lupe Fiasco
  • 'Tick Tock' by Kesha

✓ Part 5 Complete

======================================================================
All demos complete!
======================================================================