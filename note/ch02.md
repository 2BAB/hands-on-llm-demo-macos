# Hands on LLM CH02 笔记


LLM Tokenizer 的发展是自然语言处理领域一个持续演进的过程，旨在解决词汇量爆炸、未登录词（OOV）问题、多语言支持以及计算效率等挑战。其发展历程主要可以分为以下几个阶段：

---

### 1. 词级别分词 (Word-level Tokenization)

*   **概念：** 最直观的分词方法，将文本按照空格、标点符号等分隔符切分成独立的词语。
*   **发展阶段：** 早期 NLP 模型和一些传统统计模型（如 N-gram 语言模型）主要采用这种方法。
*   **进步之处：**
    *   **直观易懂：** 分词结果与人类对“词”的理解高度一致。
    *   **语义完整：** 每个词通常都具有完整的语义。
*   **局限性：**
    *   **未登录词 (OOV) 问题严重：** 训练集中未出现的新词、拼写错误、专有名词等无法识别，通常被标记为 `<UNK>` (Unknown)。
    *   **词汇量巨大：** 尤其对于形态丰富的语言（如德语、土耳其语），词形变化导致词汇量急剧膨胀，增加模型训练和推理的负担。
    *   **语言依赖性强：** 需要针对不同语言设计不同的分词规则（如中文需要专门的词典分词器）。

---

### 2. 子词级别分词 (Subword-level Tokenization)

这是 LLM 时代最核心、最重要的发展，旨在解决词级别分词的 OOV 和词汇量问题。它将词语拆分成更小的、有意义的子单元（subword）。

#### 2.1 BPE (Byte Pair Encoding)

*   **概念：** 最初用于数据压缩。在 NLP 中，它通过迭代地合并语料库中最频繁出现的相邻字符对或子词对，直到达到预设的词汇量大小。
*   **发展阶段：** 首次在神经机器翻译中被广泛应用，并被 GPT-2、早期 BERT 等模型采用。
*   **进步之处：**
    *   **有效处理 OOV：** 未知词可以被拆分成已知的子词，从而避免完全的 OOV。例如，“unbelievable”可能被拆分为“un”、“believe”、“able”。
    *   **控制词汇量：** 通过设定词汇量上限，可以有效管理模型所需的词汇表大小。
    *   **处理词形变化：** 能够将词根和词缀分离，有助于模型理解词形变化。
*   **局限性：**
    *   **依赖预分词：** 通常需要先进行空格分词，这使得它在处理某些语言（如中文、日文）时仍需额外的语言特定处理。
    *   **贪婪合并：** 合并过程是贪婪的，可能不是最优的。

#### 2.2 WordPiece

*   **概念：** 由 Google 开发，与 BPE 类似，但其合并策略略有不同。WordPiece 选择合并那些能最大化训练语料库似然度的子词对，而不是简单地选择最频繁的。它通常使用 `##` 前缀来表示一个子词是某个词的内部部分。
*   **发展阶段：** 广泛应用于 BERT、DistilBERT、T5 等 Google 系模型。
*   **进步之处：**
    *   **更优的子词选择：** 相较于纯粹的频率，基于似然度的合并通常能产生更具语义和语言学意义的子词。
    *   **在形态丰富语言中表现良好：** 对于德语、土耳其语等词形变化复杂的语言，WordPiece 往往能更好地拆分词根和词缀。
*   **局限性：**
    *   **同样依赖预分词：** 和 BPE 一样，通常需要先将文本按空格预分词。

#### 2.3 SentencePiece

*   **概念：** 由 Google 开发，是一个开源的、语言无关的文本分词器。它最大的特点是**不依赖预分词**，直接将原始文本（包括空格）视为字符序列进行处理。它可以在内部实现 BPE 或 Unigram Language Model 算法。它将空格也视为一个特殊字符（通常是 ` `），从而实现完全可逆的分词。
*   **发展阶段：** 广泛应用于 XLNet、ALBERT、LLaMA、Mistral、Gemma 等模型。
*   **进步之处：**
    *   **语言无关性：** 无需针对不同语言进行预处理，可以直接处理任何语言的文本，包括中文、日文等没有显式空格的语言。
    *   **完全可逆：** 由于将空格也视为字符进行编码，分词结果可以完美地还原回原始文本，这对于某些应用场景非常重要。
    *   **统一处理：** 避免了不同语言预分词规则带来的复杂性和不一致性。
    *   **处理未知字符：** 对于训练集中未出现的字符，可以回退到字节级别编码，确保所有输入都能被处理。

---

### 3. 字节级别分词 (Byte-level Tokenization)

*   **概念：** 将文本编码为 UTF-8 字节序列，然后对这些字节序列应用 BPE 算法。这意味着模型的基本单元不再是字符，而是字节。
*   **发展阶段：** GPT-2 首次引入了 Byte-level BPE (BBPE)，后续的 GPT-3、PaLM 等模型也沿用了这一思想。
*   **进步之处：**
    *   **零 OOV 字符：** 任何可能的文本输入（包括表情符号、罕见字符、甚至二进制数据）都可以被表示为字节序列，因此不存在未知的“字符”。
    *   **真正的通用性：** 彻底解决了字符集和编码问题，无需担心任何语言或特殊符号。
    *   **简化预处理：** 无需复杂的字符规范化或编码检测。
*   **局限性：**
    *   **序列长度增加：** 一个多字节的 UTF-8 字符（如中文汉字）会被拆分成多个字节 token，导致输入序列长度增加，从而增加计算成本。
    *   **语义粒度降低：** 字节本身通常不具备语义，模型需要从更细粒度的字节序列中学习语义。

---

### 4. 现代发展与趋势

*   **BPE-dropout：** 在 BPE 训练过程中引入随机性，每次分词时随机丢弃一些合并操作，以生成不同的分词结果，从而提高模型的鲁棒性和泛化能力。
*   **Unigram Language Model (ULM)：** SentencePiece 支持的另一种子词算法，它通过最大化语料库的对数似然来学习子词，并允许一个词有多种分段方式，在推理时选择概率最高的分段。
*   **特殊 Token：** 引入 `[CLS]` (分类)、`[SEP]` (分隔)、`[PAD]` (填充)、`[UNK]` (未知)、`[BOS]` (序列开始)、`[EOS]` (序列结束) 等特殊 token，用于模型控制和任务特定需求。
*   **效率优化：** 随着模型规模的增大，对分词速度和内存效率的要求也越来越高，出现了各种优化算法和实现。
*   **多语言统一：** 许多现代 LLM 的 tokenizer 旨在支持尽可能多的语言，通过共享子词词汇表来提高多语言任务的性能。

---

**总结：**

LLM Tokenizer 的发展历程是一个不断追求**通用性、效率和鲁棒性**的过程。从最初简单的词级别分词，到能够有效处理 OOV 和控制词汇量的子词级别分词（BPE、WordPiece），再到彻底摆脱语言依赖和字符集限制的 SentencePiece 和字节级别分词，每一步都为大型语言模型处理和理解海量、多样化的文本数据奠定了基础。这些进步使得 LLM 能够更好地应对全球范围内的多语言、多模态信息，成为强大的通用人工智能工具。


## Tokenizer 和 Embeddings 的关系

Tokenizer 和 Embeddings 是大型语言模型 (LLM) 处理文本的两个紧密相连、不可或缺的阶段。它们的关系可以概括为：**Tokenizer 将原始文本转换为模型可以理解的离散数字 ID 序列，而 Embeddings 则将这些离散 ID 映射为具有语义信息的连续向量表示，供神经网络进一步处理。**

让我们分别来看它们的作用，然后阐述它们之间的关系：

---

### 1. Tokenizer (分词器)

*   **作用：** Tokenizer 的核心任务是将原始的、人类可读的文本（字符串）切分成更小的、有意义的单元，称为 **Token**，并将每个 Token 转换为一个唯一的整数 **ID**。
*   **输出：** 一个整数 ID 序列。
*   **为什么需要它？**
    *   **机器可读性：** 计算机和神经网络无法直接处理文本字符串，它们需要数字输入。
    *   **标准化单元：** 将文本分解为统一的、可管理的单元，如词、子词或字符。
    *   **词汇表管理：** Tokenizer 维护一个词汇表 (vocabulary)，其中包含了所有已知的 Token 及其对应的 ID。这有助于控制模型的输入维度和处理未登录词 (OOV) 问题。
    *   **处理复杂语言：** 对于中文、日文等没有显式空格的语言，Tokenizer 负责识别词语边界。

---

### 2. Embeddings (嵌入层)

*   **作用：** Embeddings 层接收 Tokenizer 输出的整数 ID 序列，并将每个 ID 查找或映射到一个高维的、密集的浮点数向量。这些向量被称为 **词嵌入 (Word Embeddings)** 或 **Token Embeddings**。
*   **输出：** 一个浮点数向量序列。
*   **为什么需要它？**
    *   **语义表示：** 嵌入向量的关键在于它们能够捕捉 Token 的语义信息和上下文关系。在向量空间中，语义相似的 Token（例如，“猫”和“宠物”）会彼此靠近，而语义不相关的 Token 则会相距较远。
    *   **连续性：** 神经网络更擅长处理连续的数值数据。整数 ID 只是一个离散的标签，本身不包含任何语义信息，也无法直接进行数学运算来表示相似性。嵌入向量将离散的 ID 转换到连续的向量空间。
    *   **维度降低与效率：** 尽管嵌入向量是高维的，但它们比稀疏的 One-Hot 编码（每个 Token 对应一个维度，且只有一个维度是1，其余为0）更有效率，因为它们能用更少的维度编码更多的信息。
    *   **作为神经网络输入：** 嵌入向量是 LLM（如 Transformer 模型）的第一个输入层，后续的注意力机制、前馈网络等都将基于这些向量进行计算。

---

### Tokenizer 和 Embeddings 的关系

它们之间的关系是**顺序的、互补的，并且是 LLM 处理文本的两个连续步骤**：

1.  **Tokenizer 是 Embeddings 的前置步骤：** 原始文本首先通过 Tokenizer 转换为一系列离散的 Token ID。
    *   `原始文本` -> `Tokenizer` -> `[Token ID_1, Token ID_2, ..., Token ID_N]`

2.  **Embeddings 层接收 Tokenizer 的输出作为输入：** 每一个 Token ID 都被送入 Embeddings 层，该层会根据这个 ID 查找或生成对应的嵌入向量。
    *   `[Token ID_1, Token ID_2, ..., Token ID_N]` -> `Embeddings Layer` -> `[Vector_1, Vector_2, ..., Vector_N]`

3.  **共同构建模型的输入：** 最终，模型接收到的是一个由语义丰富的嵌入向量组成的序列，这个序列是原始文本在数值空间中的表示。通常，这些 Token Embeddings 还会与**位置编码 (Positional Embeddings)** 结合，以提供 Token 在序列中的位置信息，形成 LLM 最终的输入。

**简而言之：**

*   **Tokenizer 负责“切分”和“编号”：** 它将文本分解成基本单元，并给每个单元一个唯一的数字标签。
*   **Embeddings 负责“赋予意义”：** 它将这些数字标签转换成具有语义和上下文信息的向量，使得模型能够理解和处理这些文本单元。

没有 Tokenizer，模型无法将原始文本转换为可处理的数字形式；没有 Embeddings，这些数字形式将缺乏语义信息，模型也无法进行有效的学习和推理。它们共同构成了 LLM 理解和生成人类语言的基础。


## Word2Vec 的 Skip-gram 和 Negative sampling

好的，我们来简单解释一下 Word2Vec 领域中的 Skip-gram 和 Negative Sampling。

### Word2Vec 的核心思想

首先，Word2Vec 的目标是把词语转换成计算机可以理解的数字向量（称为词嵌入或词向量），并且这些向量能够捕捉词语的语义信息。它的核心思想是：**“一个词的意义由它周围的词决定。”** 也就是说，如果两个词经常出现在相似的语境中，那么它们的词向量就应该相似。

### 1. Skip-gram 模型

Skip-gram 是 Word2Vec 两种主要模型架构之一（另一种是 CBOW）。

*   **核心思想：** 给定一个**中心词（target word）**，预测它周围的**上下文词（context words）**。
*   **工作方式：**
    1.  你有一个句子，比如 "The quick brown fox jumps over the lazy dog"。
    2.  你选择一个中心词，比如 "fox"。
    3.  你定义一个“窗口大小”（比如窗口大小为 2），这意味着你要看中心词左右各 2 个词。那么 "fox" 的上下文词就是 "quick", "brown", "jumps", "over"。
    4.  Skip-gram 模型的目标就是：当你输入 "fox" 时，模型要尽可能高地预测出 "quick", "brown", "jumps", "over" 这些词。
*   **训练过程：** 模型通过不断调整词向量，使得中心词和其真实上下文词之间的“相似度”最大化。最终，每个词都会得到一个高质量的向量表示。
*   **为什么叫 Skip-gram？** 因为它“跳过”了中心词，去预测它周围的词。

**简单比喻：** 就像你给一个朋友看一张照片，照片里有一个人（中心词），你问他：“这个人旁边通常会出现什么？” 朋友回答出照片里这个人旁边的其他事物（上下文词），那么他就理解了这个人与周围事物的关系。

### 2. Negative Sampling (负采样)

Negative Sampling 是一种**优化技术**，它不是一个独立的模型，而是用来加速 Skip-gram（或 CBOW）模型训练过程的一种方法。

*   **问题所在：**
    *   在传统的 Skip-gram 模型中，每次训练时，模型需要计算中心词与**词汇表中所有其他词**的相似度，并更新它们的权重。
    *   如果你的词汇表有 10 万个词，那么每次训练一个样本，你就要计算 10 万次，这会非常慢，计算量巨大。
*   **Negative Sampling 的解决方案：**
    *   它把一个“多分类问题”（从 10 万个词中选出正确的上下文词）变成了一个“二分类问题”（判断一个词是不是上下文词）。
    *   **具体做法：**
        1.  **正样本（Positive Sample）：** 对于一个中心词，我们知道它有一个真实的上下文词。这个（中心词，真实上下文词）对就是一个“正样本”，我们希望模型预测它们是相关的（标签为 1）。
        2.  **负样本（Negative Samples）：** 我们随机从词汇表中抽取几个**不是**这个中心词上下文的词。这些（中心词，随机词）对就是“负样本”，我们希望模型预测它们是不相关的（标签为 0）。
        3.  **训练目标：** 模型的目标就变成了：对于正样本，预测结果接近 1；对于负样本，预测结果接近 0。
*   **效率提升：** 每次训练时，我们不再需要计算所有 10 万个词，而只需要计算 1 个正样本和 K 个负样本（K 通常很小，比如 5-20 个）。这样计算量就大大减少了，训练速度也快了很多。

**简单比喻：** 还是那个朋友看照片的例子。
*   **传统方法：** 你给朋友看一张有“狐狸”的照片，然后问他：“这张照片里，除了狐狸，还有什么？” 朋友需要列举出所有可能的事物（比如“树”、“草”、“天空”、“汽车”、“手机”……），然后告诉你哪些是照片里有的。这很麻烦。
*   **Negative Sampling 方法：** 你给朋友看一张有“狐狸”的照片，然后问他：
    *   “这张照片里有‘草’吗？”（正样本，希望回答“有”）
    *   “这张照片里有‘汽车’吗？”（负样本，希望回答“没有”）
    *   “这张照片里有‘手机’吗？”（负样本，希望回答“没有”）
    这样，朋友只需要判断几个特定的事物，而不是列举所有可能。学习效率就高多了。

### 总结

*   **Skip-gram：** 定义了 Word2Vec 的学习任务——通过中心词预测上下文词。
*   **Negative Sampling：** 是一种优化技术，通过将多分类问题转化为一系列二分类问题，大大加速了 Skip-gram 模型的训练过程，使其在处理大规模词汇表时变得可行。

两者结合，使得 Word2Vec 能够高效地从海量文本数据中学习到高质量的词向量。



## 第二章下载了那么多模型，为什么大部分只需要 tokenizer json 文件而不是完整的 safetensors 模型权重

⏺ 好问题！让我解释一下区别：

  Tokenizer vs Model 的作用

  **Tokenizer（分词器）**的作用：
  - 将文本转换为 token IDs（数字）
  - 例如："Hello world" → [101, 7592, 2088, 102]
  - 不需要模型权重，只需要词汇表（vocab）和规则

  **Model（模型权重 .safetensors）**的作用：
  - 进行实际的 AI 推理/预测
  - 例如：文本生成、分类、理解语义等
  - 需要加载几百 MB 的权重文件

  Chapter 2 中的使用场景

  让我给你看 Chapter 2 中如何使用这些 tokenizers：

⏺ Bash(grep -A 5 "show_tokens"
      /Users/2bab/Desktop/code/active/hands-on-llm-demo-macos/Chapter\ 2\ -\ Tokens\
      and\ Token\ Embeddings.ipynb | head -20)
  ⎿      "def show_tokens(sentence, tokenizer_name):\n",                   
         "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
         "    token_ids = tokenizer(sentence).input_ids\n",
     … +19 lines (ctrl+o to expand)

⏺ 看到了吗？Chapter 2 中的 show_tokens 函数：

  def show_tokens(sentence, tokenizer_name):
      tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)  # 只加载 tokenizer
      token_ids = tokenizer(sentence).input_ids                   # 只做分词
      # 打印每个 token，用于比较不同 tokenizer 的分词结果

  这里只用 tokenizer，没有加载模型！

  具体例子

  当运行 show_tokens("Hello world", "bert-base-cased") 时：

  1. 只用 tokenizer（我们下载的文件）：
    - 读取 vocab.txt（词汇表）
    - 读取 tokenizer_config.json（配置）
    - 输出：[101, 19082, 2088, 102] ✅ 完成！不需要模型
  2. 如果要用完整模型（需要 .safetensors）：
    - 还要加载 model.safetensors（436 MB）
    - 才能做语义理解、文本生成等

  对比

  | 文件                     | 大小      | 用途           | Chapter 2 需要？ |
  |------------------------|---------|--------------|---------------|
  | vocab.txt / vocab.json | ~200 KB | 词汇表，用于分词     | ✅ 需要          |
  | tokenizer_config.json  | ~1 KB   | Tokenizer 配置 | ✅ 需要          |
  | tokenizer.json         | ~600 KB | 快速 tokenizer | ✅ 需要          |
  | model.safetensors      | ~400 MB | 模型权重         | ❌ 不需要         |
  | config.json            | ~1 KB   | 模型配置         | ❌ 不需要（除非加载模型）
  |

  所以我们的下载策略是正确的：Chapter 2 只演示分词，不需要运行模型，因此只下载 
  tokenizer 文件即可。

  如果后续章节需要运行这些模型，我们可以再下载完整的模型文件。目前这样可以节省约 3-4
   GB 的磁盘空间和下载时间！ 🎯
  


## 分析 Chapter 2 的 notebook

它分为 5 个主要部分：

  📋 Chapter 2 内容结构

  Part 1: Downloading and Running An LLM (下载和运行 LLM)

  目的: 演示如何加载和使用一个完整的 LLM
  - 加载 Phi-3 模型和 tokenizer
  - 生成文本 (写一封道歉邮件)
  - 查看 tokenization 过程：
    - 输入文本 → token IDs
    - 每个 token ID 对应的文本
    - 演示 subword tokenization (例如: "apolog" + "izing", "Sub" + "ject")

  关键学习点:
  - Token IDs 是数字表示
  - 一个单词可能被分成多个 tokens

  ---
  Part 2: Comparing Trained LLM Tokenizers (比较不同的 Tokenizer)

  目的: 展示不同模型的 tokenizer 如何以不同方式分词
  - 比较 8 个不同的 tokenizers:
    a. bert-base-uncased - 不区分大小写
    b. bert-base-cased - 区分大小写
    c. gpt2 - GPT-2 的 BPE tokenizer
    d. google/flan-t5-small - T5 的 SentencePiece
    e. Xenova/gpt-4 - GPT-4 的 tiktoken
    f. bigcode/starcoder2-15b - 代码专用
    g. facebook/galactica-1.3b - 科学文本专用
    h. microsoft/Phi-3-mini-4k-instruct

  测试文本: 包含英文、大写、emoji、中文、代码、数字等

  关键学习点:
  - 不同 tokenizer 对同一文本的分词结果完全不同
  - uncased vs cased 的区别
  - 对特殊字符 (emoji、中文) 的处理差异
  - 对空格和代码的处理差异

  ---
  Part 3: Contextualized Word Embeddings From a Language Model (Like BERT) 
  (上下文化词嵌入)

  目的: 演示如何从语言模型获取词嵌入
  - 使用 DeBERTa 模型
  - 输入 "Hello world"
  - 获取每个 token 的 embedding 向量

  输出: [1, 4, 384]
  - 1 = batch size
  - 4 = tokens 数量 ([CLS], Hello, world, [SEP])
  - 384 = embedding 维度

  关键学习点:
  - 每个 token 都有一个高维向量表示
  - 这些向量是"上下文化的" (contextualized) - 同一个词在不同句子中的向量不同

  ---
  Part 4: Text Embeddings (For Sentences and Whole Documents) (句子/文档嵌入)

  目的: 演示如何将整个句子转换为单一向量
  - 使用 Sentence Transformers (all-mpnet-base-v2)
  - 输入: "Best movie ever!"
  - 输出: 单个 768 维向量

  与 Part 3 的区别:
  - Part 3: 每个 token 一个向量
  - Part 4: 整个句子一个向量

  关键学习点:
  - Sentence embeddings 用于语义搜索、相似度计算
  - 整个句子被"压缩"成一个固定长度的向量

  ---
  Part 5: Word Embeddings Beyond LLMs (LLM 之外的词嵌入)

  目的: 演示传统的静态词嵌入 (非上下文化)

  5a. GloVe Word Embeddings

  - 使用 gensim 加载 GloVe embeddings
  - 演示词向量的相似度: king 的相似词 → prince, queen, emperor

  关键学习点:
  - 静态词嵌入: 每个词只有一个固定向量
  - 与上下文化嵌入不同 (同一个词在任何上下文中向量相同)

  5b. Song Recommendation System (歌曲推荐系统)

  目的: 展示 Word2Vec 的实际应用
  - 训练 Word2Vec 模型在播放列表数据上
  - 将歌曲 ID 视为"词"
  - 播放列表视为"句子"
  - 推荐相似歌曲

  例子:
  - 输入: Metallica 的 "Fade To Black"
  - 推荐: Iron Maiden, Rush, Van Halen, Guns N' Roses...

  关键学习点:
  - Embeddings 不仅用于文本，可以用于任何序列数据
  - 相似的 items 在 embedding 空间中距离近

  ---
  📊 总结对比

  | Part | 类型               | 输入   | 输出           | 主要用途     |
  |------|------------------|------|--------------|----------|
  | 1    | LLM Tokenization | 文本   | Token IDs    | 文本生成     |
  | 2    | Tokenizer 比较     | 文本   | 不同的 tokens   | 理解分词 差异   |
  | 3    | 上下文化词嵌入          | 文本   | 每个 token 的向量 | 语言理解任务
  |
  | 4    | 句子嵌入             | 句子   | 单个向量         | 语义搜索/相似度 |
  | 5    | 静态词嵌入 + 应用       | 词/ID | 固定向量         | 推荐系统     |

  这个章节全面覆盖了从 tokenization → word embeddings → contextualized embeddings