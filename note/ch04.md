# Hands on LLM CH04 笔记

## Samples 综述

This chapter demonstrates 3 text classification approaches:

1. **Representation Models Classification**
   - Part 1a: Task-specific models (RoBERTa sentiment classifier)
   - Part 1b: Supervised classification (embeddings + logistic regression)
   - Part 1c: Zero-shot classification (using label description embeddings)
   - Part 1d: Average embeddings classification

2. **Generative Models Classification**
   - Part 2: Encoder-decoder models (FLAN-T5)
   - Part 3: ChatGPT API (optional)

## 转换 Part 2 Sample 时的一个 Critical Bug Fix

### FLAN-T5 Model Initialization Issue

**Problem Description:**
- Initial run of Part 2 showed very poor performance (only 50% accuracy)
- Model only predicted positive class, never predicted negative
- Warning appeared: `Some weights of T5ForConditionalGeneration were not initialized`

**Root Cause:**
- `download_model.py` used wrong model class to download FLAN-T5
- Used `AutoModel` instead of `AutoModelForSeq2SeqLM`
- This caused the critical `lm_head.weight` layer to be uninitialized

**Solution:**
1. Added two new functions in `download_model.py`:
   - `download_seq2seq_model()`: for T5/BART seq2seq models
   - `download_sequence_classification_model()`: for RoBERTa classification models

2. Updated model download configuration:
   ```python
   # Before (WRONG)
   ("auto_model", "google/flan-t5-small", "./model/flan-t5-small"),

   # After (CORRECT)
   ("seq2seq", "google/flan-t5-small", "./model/flan-t5-small"),
   ```

3. Removed old models and re-downloaded:
   ```bash
   rm -rf model/flan-t5-small model/twitter-roberta-base-sentiment-latest
   uv run python download_model.py
   ```

**Verification:**
- Run `test_flan_t5_fix.py` to verify model correctly outputs "positive" and "negative"
- Expected results should match original notebook: ~84% accuracy

## Performance Benchmarks (Expected Results)

| Method | Accuracy | Precision | Recall | Notes |
|--------|----------|-----------|--------|-------|
| Part 1a: RoBERTa | 80% | 0.76-0.86 | 0.72-0.88 | Task-specific model |
| Part 1b: Supervised | 85% | 0.85-0.86 | 0.85-0.86 | Embeddings + LogReg |
| Part 1c: Zero-shot | 78% | 0.77-0.78 | 0.77-0.79 | No training needed |
| Part 1d: Avg Embeddings | 84% | 0.84-0.85 | 0.84-0.85 | No classifier |
| Part 2: FLAN-T5 | 84% | 0.83-0.85 | 0.83-0.85 | Generative approach |
| Part 3: ChatGPT | 91% | 0.87-0.96 | 0.86-0.97 | Best (requires API) |

## macOS-Specific Adaptations

1. **Device Setting:** All models use `device="mps"` for Apple Silicon GPU
2. **Local Models:** Load from `./model/` directory to avoid re-downloads
3. **Runtime:**
   - Part 1a (RoBERTa): ~23 seconds
   - Part 1b (Supervised): ~15 seconds
   - Part 2 (FLAN-T5): ~40 minutes (was 3+ hours before fix)

## Key Takeaways

1. **Model class matters:** Must use correct AutoModel class when loading models
2. **Zero-shot works well:** Part 1c achieves 78% accuracy with no training
3. **Generative is powerful:** Both FLAN-T5 and ChatGPT achieve high accuracy
4. **Label descriptions matter:** Different descriptions in Part 1c affect performance
5. **Proper initialization critical:** Wrong model class can completely break inference

### Quick Test Commands

```bash
# Test only Part 2 (faster verification)
uv run python test_ch04_part2.py

# Run all parts
uv run python ch04.py

# Quick FLAN-T5 test
uv run python test_flan_t5_fix.py
```

### Lessons Learned: Adapting Jupyter Notebook to macOS Python Script

#### Critical Issues When Converting from .ipynb to .py

1. **Use Correct Model Classes for Download**

**Problem:**
- Jupyter notebook uses `pipeline()` which auto-detects model type from Hugging Face Hub
- When downloading models locally, must specify the correct model class explicitly

**Wrong Approach:**
```python
# DON'T: Generic AutoModel for all models
model = AutoModel.from_pretrained("google/flan-t5-small")
```

**Correct Approach:**
```python
# DO: Use task-specific model classes
from transformers import AutoModelForSeq2SeqLM, AutoModelForSequenceClassification

# For seq2seq models (T5, BART, etc.)
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

# For classification models (RoBERTa, BERT sentiment, etc.)
model = AutoModelForSequenceClassification.from_pretrained("cardiffnlp/twitter-roberta-base-sentiment-latest")
```

**Why It Matters:**
- Using wrong class causes critical layers (like `lm_head`) to be uninitialized
- Results in completely broken inference (50% accuracy, single-class predictions)

---

2. **Device Configuration: CUDA vs MPS**

**Problem:**
- Original notebook uses `device="cuda:0"` for NVIDIA GPUs
- macOS with Apple Silicon needs `device="mps"`

**Wrong Approach:**
```python
# DON'T: Copy CUDA settings from notebook
pipe = pipeline(model=model, device="cuda:0")  # Fails on macOS
```

**Correct Approach:**
```python
# DO: Use MPS for Apple Silicon
pipe = pipeline(model=model, device="mps")  # Works on M1/M2/M3
```

---

3. **Pipeline API Changes**

**Problem:**
- Notebook uses deprecated `return_all_scores=True`
- Modern transformers uses `top_k` parameter

**Wrong Approach:**
```python
# DON'T: Use deprecated parameter
pipe = pipeline(model=model, return_all_scores=True)  # Deprecated warning
```

**Correct Approach:**
```python
# DO: Use top_k parameter
pipe = pipeline(model=model, top_k=None)  # Returns all scores
```

---

4. **Handle Output Format Differences**

**Problem:**
- RoBERTa sentiment model has 3 classes: negative, neutral, positive
- Dataset only has 2 classes: negative (0), positive (1)
- Need to map 3-class output to 2-class labels

**Wrong Approach:**
```python
# DON'T: Assume output matches dataset directly
for output in pipe(data):
    y_pred.append(output[0]["label"])  # Wrong mapping
```

**Correct Approach:**
```python
# DO: Extract correct scores and map properly
for output in pipe(data):
    # Output: [{'label': 'negative', 'score': ...},
    #          {'label': 'neutral', 'score': ...},
    #          {'label': 'positive', 'score': ...}]
    scores_dict = {item['label']: item['score'] for item in output}
    negative_score = scores_dict.get('negative', 0)
    positive_score = scores_dict.get('positive', 0)
    assignment = np.argmax([negative_score, positive_score])
    y_pred.append(assignment)
```

---

5. **Specify Task Type Explicitly**

**Problem:**
- Pipeline might auto-detect wrong task type from local models
- Can cause incorrect tokenization or output format

**Wrong Approach:**
```python
# DON'T: Rely on auto-detection with local paths
pipe = pipeline(model="./model/flan-t5-small")
```

**Correct Approach:**
```python
# DO: Explicitly specify task type
pipe = pipeline(
    "text2text-generation",  # Explicit task
    model="./model/flan-t5-small",
    device="mps"
)
```

---

### Quick Diagnostic Checklist

When adapting notebook code to macOS Python script:

- [ ] Check model download uses correct AutoModel class
- [ ] Replace `device="cuda:0"` with `device="mps"`
- [ ] Replace `return_all_scores=True` with `top_k=None`
- [ ] Specify task type explicitly in pipeline (e.g., `"text2text-generation"`)
- [ ] Verify output format matches expected structure
- [ ] Test with small sample before running full dataset
- [ ] Check for warnings about uninitialized weights

### Verification Commands

```bash
# If you see poor results, verify models are correct:
python -c "from transformers import AutoModelForSeq2SeqLM; m = AutoModelForSeq2SeqLM.from_pretrained('./model/flan-t5-small'); print('T5 OK')"

python -c "from transformers import AutoModelForSequenceClassification; m = AutoModelForSequenceClassification.from_pretrained('./model/twitter-roberta-base-sentiment-latest'); print('RoBERTa OK')"
```
